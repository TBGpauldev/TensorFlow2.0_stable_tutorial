{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000,)\n",
      "test_label:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 25\n",
    "num_classes = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"training_data: \", x_train.shape)\n",
    "print(\"test_data: \", x_test.shape)\n",
    "print(\"training_label: \", y_train.shape)\n",
    "print(\"test_label: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000, 10)\n",
      "test_label:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train_eager = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test_eager = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train_eager = tf.reshape(tf.one_hot(y_train, 10), (-1, 10))\n",
    "y_test_eager = tf.reshape(tf.one_hot(y_test, 10), (-1, 10))\n",
    "\n",
    "print(\"training_data: \", x_train_eager.shape)\n",
    "print(\"test_data: \", x_test_eager.shape)\n",
    "print(\"training_label: \", y_train_eager.shape)\n",
    "print(\"test_label: \", y_test_eager.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "You make Dataset using `tf.data.Dataset` Class but Keras API doesn't need this dataset. If you write training loop code manually, `Dataset` class is very useful. And using keras API, you need numpy.array inputs instead of tf.Tensor. I don't know why...so you only need numpy preprocessing (or get numpy.array from tf.Tensor using numpy() method after preprocessing using function of tf).\n",
    "\n",
    "### NOTE\n",
    "This notebook we don't need 'tf.data.Dataset'. This code only just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train_eager, y_train_eager))\n",
    "    .batch(batch_size)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test_eager, y_test_eager))\n",
    "    .batch(1000)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "test_dataset = test_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN using LSTM\n",
    "In keras API, LSTM recives inputs tensor whose shape is (batch_size, seqence_length, feature_dim), and output tensor whose shape is (batch_size, fearure_dim).When you need all time sequence data, you have to give `return_sequences=True` to LSTM's constractor. Generally, when you stack LSTM's, you need all sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_size=10, num_layers=2, num_classes=10):\n",
    "        super(RNN, self).__init__(name='mnist_rnn')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = self.get_lstm_layers(hidden_size, num_layers)            \n",
    "        self.fc = L.Dense(num_classes, activation=\"softmax\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lstm_layers(hidden_size, num_layers):\n",
    "        lstm_layers = []\n",
    "        # we need all sequence data. write return_sequences=True! \n",
    "        for i in range(num_layers-1):\n",
    "            lstm_layers.append(\n",
    "                L.LSTM(units=hidden_size, return_sequences=True)\n",
    "            )\n",
    "        # the final layer return only final sequence\n",
    "        # if you need all sequences, you have to write return_sequences=True.\n",
    "        lstm_layers.append(L.LSTM(units=hidden_size))\n",
    "        return tf.keras.Sequential(lstm_layers)\n",
    "        \n",
    "    def call(self, x):        \n",
    "        # Forward propagate LSTM\n",
    "        out = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_rnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    multiple                  2400      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  110       \n",
      "=================================================================\n",
      "Total params: 2,510\n",
      "Trainable params: 2,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Eager Execution initialize parameters when using model.call()\n",
    "model(x_train_eager[:50])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 4s 81us/sample - loss: 2.2043 - accuracy: 0.2040 - val_loss: 2.0070 - val_accuracy: 0.3113\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 1.7273 - accuracy: 0.4334 - val_loss: 1.4141 - val_accuracy: 0.5475\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 1.2682 - accuracy: 0.5857 - val_loss: 1.0611 - val_accuracy: 0.6589\n",
      "Epoch 4/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.9947 - accuracy: 0.6739 - val_loss: 0.8703 - val_accuracy: 0.7171\n",
      "Epoch 5/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.8464 - accuracy: 0.7215 - val_loss: 0.7709 - val_accuracy: 0.7457\n",
      "Epoch 6/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.7627 - accuracy: 0.7479 - val_loss: 0.7020 - val_accuracy: 0.7643\n",
      "Epoch 7/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.7093 - accuracy: 0.7654 - val_loss: 0.6565 - val_accuracy: 0.7806\n",
      "Epoch 8/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.6671 - accuracy: 0.7777 - val_loss: 0.6208 - val_accuracy: 0.7961\n",
      "Epoch 9/25\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.79 - 1s 23us/sample - loss: 0.6347 - accuracy: 0.7918 - val_loss: 0.5906 - val_accuracy: 0.8070\n",
      "Epoch 10/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.6054 - accuracy: 0.8046 - val_loss: 0.5669 - val_accuracy: 0.8170\n",
      "Epoch 11/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.5768 - accuracy: 0.8164 - val_loss: 0.5408 - val_accuracy: 0.8277\n",
      "Epoch 12/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.5527 - accuracy: 0.8270 - val_loss: 0.5218 - val_accuracy: 0.8369\n",
      "Epoch 13/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.5326 - accuracy: 0.8335 - val_loss: 0.5016 - val_accuracy: 0.8430\n",
      "Epoch 14/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.5105 - accuracy: 0.8411 - val_loss: 0.4886 - val_accuracy: 0.8470\n",
      "Epoch 15/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.4932 - accuracy: 0.8461 - val_loss: 0.4660 - val_accuracy: 0.8563\n",
      "Epoch 16/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.4745 - accuracy: 0.8524 - val_loss: 0.4516 - val_accuracy: 0.8606\n",
      "Epoch 17/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.4609 - accuracy: 0.8564 - val_loss: 0.4416 - val_accuracy: 0.8602\n",
      "Epoch 18/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.4489 - accuracy: 0.8600 - val_loss: 0.4265 - val_accuracy: 0.8693\n",
      "Epoch 19/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.4386 - accuracy: 0.8629 - val_loss: 0.4233 - val_accuracy: 0.8708\n",
      "Epoch 20/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.4273 - accuracy: 0.8661 - val_loss: 0.4076 - val_accuracy: 0.8748\n",
      "Epoch 21/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.4149 - accuracy: 0.8703 - val_loss: 0.3989 - val_accuracy: 0.8778\n",
      "Epoch 22/25\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.4054 - accuracy: 0.8726 - val_loss: 0.3920 - val_accuracy: 0.8791\n",
      "Epoch 23/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.3949 - accuracy: 0.8767 - val_loss: 0.3780 - val_accuracy: 0.8830\n",
      "Epoch 24/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.3846 - accuracy: 0.8802 - val_loss: 0.3752 - val_accuracy: 0.8841\n",
      "Epoch 25/25\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.3765 - accuracy: 0.8827 - val_loss: 0.3654 - val_accuracy: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f470306198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train_eager.numpy(), \n",
    "          y=y_train_eager.numpy(), \n",
    "          validation_split=0.2, \n",
    "          epochs=num_epochs,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accracy:  0.8806\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x=x_test_eager.numpy(), \n",
    "                                     y=y_test_eager.numpy(), verbose=False)\n",
    "\n",
    "print(\"test_accracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
