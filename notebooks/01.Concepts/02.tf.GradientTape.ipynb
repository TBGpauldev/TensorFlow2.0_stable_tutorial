{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# tf.GradientTape"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n# To start eager execution (this must be top of code)\ntf.executing_eagerly()\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus: tf.config.experimental.set_memory_growth(gpus[0], True)\n"},{"cell_type":"markdown","metadata":{},"source":["### Create Tensors"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'tensorflow.python.framework.ops.EagerTensor'>\n","tf.Tensor(1.0, shape=(), dtype=float32)\n"]}],"source":"x = tf.convert_to_tensor(1.)\nw = tf.convert_to_tensor(2.)\nb = tf.convert_to_tensor(3.)\n\nprint(type(x))\nprint(x)"},{"cell_type":"markdown","metadata":{},"source":["### Build a computational graph for Automatic differentiation.\n","When you focus the $x$ of two functions, \n","\n","$$\n","y(x) = w  x + b\n","$$ \n","and\n","$$ \n","z(x) = w  x^2 + b x\n","$$\n","you can write the code of \"build a computational graph for automatic differentiation\" as below."]},{"cell_type":"markdown","metadata":{},"source":["### tf.GradientTape\n","텐서플로는 자동 미분(주어진 입력 변수에 대한 연산의 그래디언트(gradient)를 계산하는 것)을 위한 tf.GradientTape API를 제공합니다. tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"합니다. 그 다음 텐서플로는 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산합니다."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(2.0, shape=(), dtype=float32)\n","tf.Tensor(7.0, shape=(), dtype=float32)\n"]}],"source":"with tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n    y = w * x + b\n    z = w* x**2 + b * x\n    \n# dy/dx = 2\n# dz/dx = 4 * x + 3  (now x=1 so dz/dx = 7)\n# 입력 텐서 x에 대한 y의 도함수\ndy_dx = tape.gradient(y, x)\n# 입력 텐서 x에 대한 z의 도함수\ndz_dx = tape.gradient(z, x)\n    \nprint(dy_dx)\nprint(dz_dx)"},{"cell_type":"markdown","metadata":{},"source":["### linear model\n","tf.keras 모듈을 사용하여 간단한 Liniear model을 만들어 보겠습니다.\n","$$\n"," y_i = Wx_i + b\n","$$"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: \n"," (10, 3)\n","\n","weight: \n"," (3, 2)\n","\n","bias:\n"," (2,)\n","\n","output shape:\n"," (10, 2)\n"]}],"source":"x = tf.random.normal(shape=[10, 3])\ny = tf.random.normal(shape=[10, 2])\n\n# tf.keras.layers.Dense needs only output dimension.\n# When tf.keras.layers get input to calculate output at the first time,\n# the input dimension is determined.\nlinear = tf.keras.layers.Dense(units=2,input_shape=(10,3))\npredict_y = linear(x)\n\nprint(\"Input: \\n\", x.shape, end=\"\\n\\n\")\nprint(\"weight: \\n\", linear.weights[0].shape, end=\"\\n\\n\")\nprint(\"bias:\\n\", linear.weights[1].shape, end=\"\\n\\n\")\nprint(\"output shape:\\n\", predict_y.shape)"},{"cell_type":"markdown","metadata":{},"source":["#### loss function\n","TensorFlow eager execution has similar API to PyTorch, however the implementation of \"Build a computational graph for Automatic differentiation\"  is a little diferrent.\n","At PyTorch, Tensor itself holds calculation graph, and have the method for automatic differentiation. On the other hand, at TensorFlow eager execution, computational graph is keeped by some functions (for example, `tf.GradientTape()`). "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: \n"," tf.Tensor(\n","[3.000647   0.0633526  0.21499693 2.5319018  5.310877   1.7093294\n"," 0.48144472 0.23076423 1.005112   0.92788976], shape=(10,), dtype=float32)\n","\n","weight grads: \n"," tf.Tensor([3.5041962 1.1919472], shape=(2,), dtype=float32)\n","\n","weight instances: \n"," tf.Tensor([-3.6995046 -1.933562 ], shape=(2,), dtype=float32)\n","\n","bias grads: \n"," tf.Tensor(-1.6289247, shape=(), dtype=float32)\n","\n","bias instances: \n"," tf.Tensor(-0.9769095, shape=(), dtype=float32)\n","\n"]}],"source":"def loss_fn(model, x, y):\n    predict_y = model(x)\n    return tf.keras.losses.mean_squared_error(predict_y, y)\n\nwith tf.GradientTape() as tape:\n    loss = loss_fn(model=linear, x=x, y=y)\n    grads = tape.gradient(loss, linear.trainable_variables)\n\nprint(\"loss: \\n\", loss, end=\"\\n\\n\")\nprint(\"weight grads: \\n\", grads[0][0], end=\"\\n\\n\")\nprint(\"weight instances: \\n\", grads[0][1], end=\"\\n\\n\")\nprint(\"bias grads: \\n\", grads[1][0], end=\"\\n\\n\")\nprint(\"bias instances: \\n\", grads[1][1], end=\"\\n\\n\")"},{"cell_type":"markdown","metadata":{},"source":["#### Optimizing\n","We aim to decrese loss value with update parameters as below. \n","$$\n","\\begin{align}\n","W & \\leftarrow W - \\epsilon \\frac{dLoss(W)}{dW}\\\\\\\n","b & \\leftarrow b - \\epsilon \\frac{dLoss(W)}{db}\n","\\end{align}\n","$$\n","\n","where $\\epsilon$ is learning rate.\n","\n","After understanding this code, you are able to write training loop code."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"# initialize optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss:  tf.Tensor(15.4763155, shape=(), dtype=float32)\n","loss:  tf.Tensor(14.69506, shape=(), dtype=float32)\n"]}],"source":"# initial loss value of sum of all data.\nwith tf.GradientTape() as tape:\n    loss = loss_fn(model=linear, x=x, y=y)\n    grads = tape.gradient(loss, linear.trainable_variables)\nprint(\"loss: \", tf.reduce_sum(loss))\n\n# update prameters using grads\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\noptimizer.apply_gradients(zip(grads, linear.trainable_variables))\n\n# loss value after update (may be less than before update)\nwith tf.GradientTape() as tape:\n    loss = loss_fn(model=linear, x=x, y=y)\n    grads = tape.gradient(loss, linear.trainable_variables)\nprint(\"loss: \", tf.reduce_sum(loss))"},{"cell_type":"markdown","metadata":{},"source":["### Data\n","#### Convert to tf.Tensor from numpy.ndarray"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'>\n","[[ 0.07104778  0.24263894 -1.32652316]\n"," [-0.33225697  0.0959315   0.01543784]\n"," [ 0.3612971   0.34421558 -0.27386095]]\n","<class 'tensorflow.python.framework.ops.EagerTensor'>\n","tf.Tensor(\n","[[ 0.07104778  0.24263894 -1.32652316]\n"," [-0.33225697  0.0959315   0.01543784]\n"," [ 0.3612971   0.34421558 -0.27386095]], shape=(3, 3), dtype=float64)\n"]}],"source":"X_numpy = np.random.randn(3, 3)\nprint(type(X_numpy))\nprint(X_numpy)\n\nX_tensor = tf.convert_to_tensor(X_numpy)\nprint(type(X_tensor))\nprint(X_tensor)"},{"cell_type":"markdown","metadata":{},"source":["#### conver to numpy.array from tf.Tensor\n","주의할 점은 eager execution 모드에서만 .numpy()호출이 가능한 점 입니다."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'tensorflow.python.framework.ops.EagerTensor'>\n","tf.Tensor(\n","[[ 1.7827262   0.9076068  -0.4188783 ]\n"," [-1.8647549   0.4820718  -0.29639027]\n"," [ 0.24610497 -0.41468096  1.2479974 ]], shape=(3, 3), dtype=float32)\n","<class 'numpy.ndarray'>\n","[[ 1.7827262   0.9076068  -0.4188783 ]\n"," [-1.8647549   0.4820718  -0.29639027]\n"," [ 0.24610497 -0.41468096  1.2479974 ]]\n"]}],"source":"X_tensor = tf.random.normal(shape=[3, 3])\nprint(type(X_tensor))\nprint(X_tensor)\n\nX_numpy = X_tensor.numpy()\nprint(type(X_numpy))\nprint(X_numpy)"},{"cell_type":"markdown","metadata":{},"source":["### tf.Dataset pipline"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}