{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Bidrectional Recurrent Neural Network using KerasAPI"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n# To start eager execution (this must be top of code)\ntf.executing_eagerly()\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus: tf.config.experimental.set_memory_growth(gpus[0], True)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training_data:  (60000, 28, 28)\n","test_data:  (10000, 28, 28)\n","training_label:  (60000,)\n","test_label:  (10000,)\n"]}],"source":"# Hyper parameters\nnum_epochs = 5\nnum_classes = 10\nbatch_size = 512\nlearning_rate = 0.001\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nprint(\"training_data: \", x_train.shape)\nprint(\"test_data: \", x_test.shape)\nprint(\"training_label: \", y_train.shape)\nprint(\"test_label: \", y_test.shape)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training_data:  (60000, 28, 28)\n","test_data:  (10000, 28, 28)\n","training_label:  (60000, 10)\n","test_label:  (10000, 10)\n"]}],"source":"x_train_eager = tf.convert_to_tensor(x_train, dtype=tf.float32)\nx_test_eager = tf.convert_to_tensor(x_test, dtype=tf.float32)\ny_train_eager = tf.reshape(tf.one_hot(y_train, 10), (-1, 10))\ny_test_eager = tf.reshape(tf.one_hot(y_test, 10), (-1, 10))\n\nprint(\"training_data: \", x_train_eager.shape)\nprint(\"test_data: \", x_test_eager.shape)\nprint(\"training_label: \", y_train_eager.shape)\nprint(\"test_label: \", y_test_eager.shape)"},{"cell_type":"markdown","metadata":{},"source":["### DataSet\n","You make Dataset using `tf.data.Dataset` Class but Keras API doesn't need this dataset. If you write training loop code manually, `Dataset` class is very useful. And using keras API, you need numpy.array inputs instead of tf.Tensor. I don't know why...so you only need numpy preprocessing (or get numpy.array from tf.Tensor using numpy() method after preprocessing using function of tf).\n","\n","### NOTE\n","This notebook we don't need 'tf.data.Dataset'. This code only just for reference."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"train_dataset = (\n    tf.data.Dataset.from_tensor_slices((x_train_eager, y_train_eager))\n    .batch(batch_size)\n    .shuffle(10000)\n)\ntrain_dataset = train_dataset.repeat()"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"test_dataset = (\n    tf.data.Dataset.from_tensor_slices((x_test_eager, y_test_eager))\n    .batch(1000)\n    .shuffle(10000)\n)\ntest_dataset = test_dataset.repeat()"},{"cell_type":"markdown","metadata":{},"source":["### RNN using LSTM\n","In keras API, LSTM recives inputs tensor whose shape is (batch_size, seqence_length, feature_dim), and output tensor whose shape is (batch_size, fearure_dim).When you need all time sequence data, you have to give `return_sequences=True` to LSTM's constractor. Generally, when you stack LSTM's, you need all sequence data.\n","\n","We use  just only `tf.keras.layers.Bidirectional` for using Bidrectional LSTM."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"class RNN(tf.keras.Model):\n    def __init__(self, hidden_size=10, num_layers=2, num_classes=10):\n        super(RNN, self).__init__(name='mnist_rnn')\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = self.get_lstm_layers(hidden_size, num_layers)            \n        self.fc = L.Dense(num_classes, activation=\"softmax\")\n    \n    @staticmethod\n    def get_lstm_layers(hidden_size, num_layers):\n        lstm_layers = []\n        # we need all sequence data. write return_sequences=True! \n        for i in range(num_layers-1):\n            lstm_layers.append(\n                L.Bidirectional(\n                    L.LSTM(units=hidden_size, \n                                         return_sequences=True)\n                )\n            )\n        # the final layer return only final sequence\n        # if you need all sequences, you have to write return_sequences=True.\n        lstm_layers.append(\n            L.Bidirectional(\n                L.LSTM(units=hidden_size)\n            )\n        )\n        return tf.keras.Sequential(lstm_layers)\n        \n    def call(self, x):        \n        # Forward propagate LSTM\n        out = self.lstm(x)\n        out = self.fc(out)\n        return out"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"mnist_rnn\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","sequential (Sequential)      multiple                  5600      \n","_________________________________________________________________\n","dense (Dense)                multiple                  210       \n","=================================================================\n","Total params: 5,810\n","Trainable params: 5,810\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":"model = RNN()\noptimizer = tf.keras.optimizers.Adam(learning_rate)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=[\"accuracy\"])\n\n# Eager Execution initialize parameters when using model.call()\nmodel(x_train_eager[:50])\n\nmodel.summary()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 48000 samples, validate on 12000 samples\n","Epoch 1/5\n","48000/48000 [==============================] - 17s 356us/sample - loss: 2.0521 - accuracy: 0.3268 - val_loss: 1.6240 - val_accuracy: 0.5416\n","Epoch 2/5\n","48000/48000 [==============================] - 2s 44us/sample - loss: 1.2421 - accuracy: 0.6534 - val_loss: 0.9187 - val_accuracy: 0.7492\n","Epoch 3/5\n","48000/48000 [==============================] - 2s 38us/sample - loss: 0.7856 - accuracy: 0.7755 - val_loss: 0.6455 - val_accuracy: 0.8100\n","Epoch 4/5\n","48000/48000 [==============================] - 2s 37us/sample - loss: 0.5988 - accuracy: 0.8243 - val_loss: 0.5273 - val_accuracy: 0.8424\n","Epoch 5/5\n","48000/48000 [==============================] - 2s 37us/sample - loss: 0.5001 - accuracy: 0.8534 - val_loss: 0.4532 - val_accuracy: 0.8643\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x2dac5e0dc50>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"model.fit(x=x_train_eager.numpy(), \n          y=y_train_eager.numpy(), \n          validation_split=0.2, \n          epochs=num_epochs,\n          batch_size=batch_size)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["test_accracy:  0.8725\n"]}],"source":"test_loss, test_acc = model.evaluate(x=x_test_eager.numpy(), \n                                     y=y_test_eager.numpy(),verbose=False)\n\nprint(\"test_accracy: \", test_acc)"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}