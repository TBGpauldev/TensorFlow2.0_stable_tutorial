{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Basic Tensorflow 2.0 Coding Style"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import tensorflow as tf\ntf.executing_eagerly()\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus: tf.config.experimental.set_memory_growth(gpus[0], True)"},{"cell_type":"markdown","metadata":{},"source":["## Make DataSet"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"'''\nIf you have numpy data, you can use\ntensor_data = tf.convert_to_tensor(numpy_data, dtype=tf.float32)\nfor translation into tf.Tensor.\n'''\n# example training data\nfeature = tf.random.normal(shape=[50000, 1000])\ntarget = tf.random.normal(shape=[50000, 10])\n\n# example validation data\nval_feature = tf.random.normal(shape=[10000, 1000])\nval_target = tf.random.normal(shape=[10000, 10])\n\n# example test data\ntest_feature = tf.random.normal(shape=[5000, 1000])\ntest_target = tf.random.normal(shape=[5000, 10])\n\n\n# make dataset\ndataset = tf.data.Dataset.from_tensor_slices((feature, target))\nval_dataset = tf.data.Dataset.from_tensor_slices((val_feature, val_target))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_feature, test_target))"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["<TensorSliceDataset shapes: ((1000,), (10,)), types: (tf.float32, tf.float32)>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"# A dataset have shape information except batchsize and data type.\ndataset"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# Training data should be shuffled every epoch.\n# 10000 is buffer size.\ndataset = dataset.shuffle(10000)\n\n# For mini-batch training.\n# 256 is batch size.\ndataset = dataset.batch(256)\n\n# Of course we can write same code as follows\n# dataset = dataset.shuffle(10000).batch(256)\n\n# validation data and test data do NOT need shuffle.\n# batch size is as big as possible.\nval_dataset = val_dataset.batch(10000)\ntest_dataset = test_dataset.batch(5000)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["<BatchDataset shapes: ((None, 1000), (None, 10)), types: (tf.float32, tf.float32)>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"# dataset is set for batch training.\ndataset"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"class MyNet(tf.keras.Model):\n    '''\n    We use basically tf.keras.Model for making network.\n    This class will manage layers and that's trainable parameters.\n    '''\n    def __init__(self):\n        super(MyNet, self).__init__()\n        \n        \n        # We can use tf.keras.Sequential \n        # which has a role of putting together some layers.\n        # This class inherits tf.keras.Model, so this can manege parameters too.\n        # This class only receive layers.Layer class.\n        # (Note that tf.keras.Sequential receive tf.keras.layers.ReLU())\n       \n        self.layer1 = tf.keras.Sequential([\n            tf.keras.layers.Dense(1024),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.BatchNormalization(axis=-1),\n            tf.keras.layers.Dropout(rate=0.2),\n        ])\n        \n        # Of course we can write some layers separately.\n        \n        self.dense = tf.keras.layers.Dense(256)\n        self.bn = tf.keras.layers.BatchNormalization(axis=-1)\n        self.do = tf.keras.layers.Dropout(rate=0.2)\n        \n        self.dense_output = tf.keras.layers.Dense(10)\n    \n    # tf.function is jit compiler which translate python code into TF graph.\n    @tf.function\n    def call(self, x, training=False):\n        # tf.keras.Sequential class have training propaty\n        # which manege behavior of dropout and batchnormalization etc.\n        h = self.layer1(x, training=training)\n        \n        h = self.dense(h)\n        # we can use tf.nn.relu function instead of tf.keras.layers.ReLU()\n        h = tf.nn.relu(h)\n\n        # BatchNormalization and Dropout class also have training property.\n        h = self.bn(h, training=training)\n        h = self.do(h, training=training)\n        \n        return self.dense_output(h)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"my_net_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","sequential_2 (Sequential)    multiple                  1029120   \n","_________________________________________________________________\n","dense_7 (Dense)              multiple                  262400    \n","_________________________________________________________________\n","batch_normalization_5 (Batch multiple                  1024      \n","_________________________________________________________________\n","dropout_5 (Dropout)          multiple                  0         \n","_________________________________________________________________\n","dense_8 (Dense)              multiple                  2570      \n","=================================================================\n","Total params: 1,295,114\n","Trainable params: 1,292,554\n","Non-trainable params: 2,560\n","_________________________________________________________________\n"]}],"source":"model = MyNet()\n# test execution.\nmodel(tf.random.normal(shape=[1, 1000]))\n\nmodel.summary()"},{"cell_type":"markdown","metadata":{},"source":["## Training by hand"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"optimizer = tf.keras.optimizers.Adam()\nloss_fn = tf.keras.losses.MeanSquaredError()"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"@tf.function\ndef train_step(feature, target):\n\n    with tf.GradientTape() as tape:\n        y_pred = model(feature, training=True)\n        loss = loss_fn(target, y_pred)\n    \n    grads = tape.gradient(loss, model.variables)\n    optimizer.apply_gradients(zip(grads, model.variables))\n    \n    return loss\n\n@tf.function\ndef val_step(feature, target):\n    \n    y_pred = model(feature)\n    loss = loss_fn(target, y_pred)\n    \n    return loss"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------epoch 196--------\n","loss: 0.46282273530960083,  val_loss: 1.203696846961975\n","----------epoch 196--------\n","loss: 0.45190444588661194,  val_loss: 1.2077049016952515\n","----------epoch 196--------\n","loss: 0.44128596782684326,  val_loss: 1.2036551237106323\n","----------epoch 196--------\n","loss: 0.43410274386405945,  val_loss: 1.2066048383712769\n","----------epoch 196--------\n","loss: 0.4279598295688629,  val_loss: 1.2057843208312988\n","----------epoch 196--------\n","loss: 0.41745299100875854,  val_loss: 1.2026267051696777\n","----------epoch 196--------\n","loss: 0.4114142954349518,  val_loss: 1.2086076736450195\n","----------epoch 196--------\n","loss: 0.40470507740974426,  val_loss: 1.2065753936767578\n","----------epoch 196--------\n","loss: 0.39911866188049316,  val_loss: 1.2081482410430908\n","----------epoch 196--------\n","loss: 0.3923533260822296,  val_loss: 1.207283616065979\n"]}],"source":"for i in range(10):\n    \n    running_loss = 0\n    running_val_loss = 0\n    \n    for i, (batch_feature, batch_target) in enumerate(dataset):\n        loss_ = train_step(batch_feature, batch_target)\n        running_loss += loss_\n        \n    for j, (batch_feature, batch_target) in enumerate(val_dataset):\n        loss_ = val_step(batch_feature, batch_target)\n        running_val_loss += loss_\n        \n    print(\"----------epoch {}--------\".format(i+1))\n    print(\"loss: {},  val_loss: {}\".format(running_loss/(i+1), \n                                           running_val_loss/(j+1)))"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}